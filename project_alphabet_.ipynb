{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f215173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2 #opencv\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba9c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "#Creating the dimensions for the ROI...\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd83f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224fa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(),\n",
    "    cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b0bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "A= r'C:\\gesture\\train'\n",
    "num_frames = 0\n",
    "element = A\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),2)\n",
    "        \n",
    "        # Checking if the hand is actually detected by counting the number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element),\n",
    "            (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\"\n",
    "      + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "      (0,0,255), 2)\n",
    "            \n",
    "          \n",
    "          #displaying thresholded images \n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                cv2.imwrite(r\"C:\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" +\n",
    "                str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400),\n",
    " cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"Hand sign recognition\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing the camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f829c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 15 classes.\n",
      "Found 0 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'C:\\gesture\\train'\n",
    "test_path = r'C:\\gesture\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=15,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=15, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85c86fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 64, 64, 3)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    def plt(images_arr):\n",
    "        fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "        axes = axes.flatten()\n",
    "        for img, ax in zip( images_arr, axes):\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513faa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f045b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51dc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)\n",
    "imgs, labels = next(train_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2520de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model(r\"best_model.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "# print(len(scores))\n",
    "\n",
    "#Once the model is fitted we save the model using model.save() function.\n",
    "model.save('best_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74389964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "#print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f711ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model(r\"C:\\Users\\addy1\\AppData\\Local\\Programs\\Python\\Python39\\best_model.h5\")\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1248c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    \n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #Fetching contours in the frame (These contours can be of hand or any other object in foreground) ...\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If length of contours list = 0, means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment(max contour) and the thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa33d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        # segmenting the hand region\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "\n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "                        \n",
    "               # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.putText(frame_copy, \"sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f68571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
